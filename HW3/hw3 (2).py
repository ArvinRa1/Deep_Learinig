# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZE9QmgyEpXa0D-WNER6Ww2jw70OlSgxS

#**HW 3: Convolutional Neural Networks**

In this homework, we will explore how to develop a simple Convolutional Neural Network for image classification. We will use the Fashion Minst dataset. In the first part, we will learn how to develop a simple CNN, while in the second part we will explore the impact of various hyper-parameters in the learning performances.

##Exercise 3.1: Simple CNN

Let's start by importing Tensorflow, Keras and Numpy
"""
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)

"""###Load dataset:
### Load Data: Fashion MNIST dataset

We will use the Fashion MNIST dataset, a dataset of Zalando's articles. Each sample is a 28x28 pixels grayscale image, associated with a label from 10 classes:

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

Each pixel intensity is represented by a uint8 (byte) from 0 to 255.
We will divide the dataset in training, validation and test set. As you already know, the training set will be used to train the model, the validation set will be used to perform model selection and finally, the test set will be used to asses the performance of the deep network.

Since we will use a [2DConv](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) we have to provide also a new dimension of the input that represents the number of channels (that in grey scale image is just one). Therefore we use [tf.expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims) to transform each image from a matrix to a 3-dimensional tensor. Finaly we have to normalize the input data.
"""

fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = (
    fashion_mnist.load_data())  # The dataset is already divede in test and training..

# We extract the first 5000 samples of the training set, to use them as the validation set
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

# [TO COMPLETE]: define X_train_scaled, X_valid_scaled and X_test_scaled, the sets that contains
# Hint: For each feature (pixel intensity), you must subtract the mean() of that
# feature (across all instances, so use axis=0) and divide by its standard
# deviation (std(), again axis=0)
X_train_mean = np.mean(X_train)
X_train_std = np.std(X_train)
X_train = (X_train - X_train_mean) / X_train_std  # [TO COMPLETE]
X_valid = (X_valid - X_train_mean) / X_train_std  # [TO COMPLETE]
X_test = (X_test - X_train_mean) / X_train_std  # [TO COMPLETE]

# Add one dimension to manage the channel
X_train = tf.expand_dims(X_train, 3)
X_valid = tf.expand_dims(X_valid, 3)
X_test = tf.expand_dims(X_test, 3)


# Make sure you compute the means and standard deviations on the training set,
# and use these statistics to scale the training set, the validation set and the
# test set

# Make sure you compute the means and standard deviations on the training set,
# and use these statistics to scale the training set, the validation set and the
# test set

###Define the model
def plot_learning_acc_and_loss(history):
    pd.DataFrame(history.history).plot(figsize=(8, 5))
    plt.grid(True)
    plt.show()


def plot_loss(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.epoch, history.history['loss'])
    plt.plot(history.epoch, history.history['val_loss'])
    plt.title('loss')


def plot_accuracy(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.epoch, history.history['accuracy'])
    plt.plot(history.epoch, history.history['val_accuracy'])
    plt.title('accuracy')


"""###Visualize Weights

An interesting thing to do is to visualize the learned weights for the convolutional layer. We have 32 kernels of size 3x3, we can just plot them as images, mapping the weight values to grayscale.
"""

# Weights for the first convolutional layer


"""They might be a bit hard to interpret, but it seems that the various filters have learned to detect various corners and edges.

### [TO COMPLETE] Deep CNN
Let's consider a deeper model, more precily in this exercise we consider a model composed of:
* One 2D convolutional layer with kernel size 3x3 and 32 output filters/features, that use ReLu activation function
* a Max Pooling layer (2D) of size 2x2 
* One 2D convolutional layer with kernel size 2x2 and 16 output filters/features, that use ReLu activation function
* a Max Pooling layer (2D) of size 2x2
* a Flatten layer
* a final Dense layer with 10 output neurons (one per class), and with the "softmax" activation function
"""
histories = []
optimizers = ['Adam']
learning_rates = [1e-4, 1e-2, 1e-3, 1e-5]
n_units = [32, 16, 64, 128]
batches = [32, 64, 128, 256]
for batch in batches:
    model = keras.models.Sequential([
        keras.layers.Conv2D(filters=128, kernel_size=[2, 2], padding="same", activation="relu",
                            input_shape=[28, 28, 1]),
        keras.layers.MaxPool2D(pool_size=[2, 2]),
        keras.layers.Conv2D(filters=128, kernel_size=[2, 2], activation='relu', padding='same'),
        keras.layers.MaxPool2D(pool_size=(2, 2)),
        keras.layers.Flatten(),
        keras.layers.Dense(10, activation="softmax")
    ])
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  metrics=["accuracy"])

    callbacks = [keras.callbacks.EarlyStopping(patience=2)]

    history = model.fit(X_train, y_train, batch_size=batch,
                        validation_data=(X_valid, y_valid), epochs=30,
                        callbacks=callbacks)
    histories.append(history)
    print("evaluation on test set", batch)
    model.evaluate(X_test, y_test)

"""
param_distribs = {
    "n_units": [32, 16, 64, 128],  # [TO COMPLETE] insert a list that contains few (2 or 3) reasonable value
    "learning_rate": [1e-4, 1e-2, 1e-3, 1e-5]
    # [TO COMPLETE] insert a list that contains few (2 or 3) reasonable values
    # Check how the time required to perform GRID search increases when increasing the number of values for each hyper-parameter.
}


from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(keras_reg, param_distribs)
"""

"""
grid_search.fit(X_train.numpy(), y_train, epochs=10,
                # The number of epochs can be modified (check what happens by increasing it)
                validation_data=(X_valid.numpy(), y_valid), verbose=0)
print(grid_search.best_params_)

model = grid_search.best_estimator_.model
model.evaluate(X_test, y_test)
# plot_loss(history)
# plot_accuracy(history)

"""

"""## [TO COMPLETE] Exercise 3.2: Develop a better CNN
Let's develop a network that performs better than the very simple one above. This exercise aims to explore how much the various hyper-parameters influence the classification capability of the model. 

**[TO COMPLETE]**: Your task is to modify some of the hyper-parameters of the previous exercise's network and compare the results. At least one of the models you try should have an improvement in the test set results (generalization) over the result of the model used in the previous exercise.
In the cell below report only the code of the **best model** that you can find. In addtion, print out its result on the test set, and plot the accuracy and the loss trends in the notebook you return.
Moreover, for each setup you test, analyze and discuss the obtained results briefly in the last cells at the bottom.

Hint: Each reparameterization should change a different aspect in the network, while the rest of the parameters would stay the same. 
Example parameters to try to change (we suggest to test at least one re-parametrization for each of these categories):

*    number of layers or neurons or filters dimension
*   activation functions
*   epochs
*   batch sizes
*   optimizer, see TensorFlow documentation on [optimizers](https://https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)
*   max-pooling on/off on certain layers, or pool size

For what concerns the optimizer, as you can see in the 'compile' method [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) is it possible to pass as 'optimizer ' attribute's value a string (the name of optimizer) or an optimizer instance.

Notice that changing the final layer's softmax activation plus the categorical_crossentropy loss requires some consideration. Don't do it unless you have a good plan.
"""

# [TO COMPLETE]

"""### [TO COMPLETE] Example of tests discussion
The best model that I found ...[TO COMPLETE]

The achieved accuracy in the test set is ...[TO COMPLETE]

Discussion:
[TO COMPLETE]

Besides, I tested also other models: 
* [TO COMPLETE]
* ..


Discussion:
[TO COMPLETE]
"""
