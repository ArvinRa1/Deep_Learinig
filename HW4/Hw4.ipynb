{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hw4.ipynb","provenance":[{"file_id":"11CmjcOpjWeZSOOr7TEmV5HbFIYx2EZe3","timestamp":1588863979233},{"file_id":"1S2IocRQdDE09i5KR6ZCchdji6SEbVUTc","timestamp":1588836539424},{"file_id":"1gYhf63p6lyrvoXLQ6NipjpfNADWVKRvk","timestamp":1588636925965}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S4Iui5UetZvC"},"source":["#**HW4 : Recurrent Neural Networks**\n","In this homework, we will explore how to develop a simple Recurrent Neural Network (RNN) for sentiment analysis. As a dataset, we will use the IMDB dataset. It contains the text of some reviews and the sentiment given by the authors of the reviews (positive review or negative review). The input to the RNN is the sequence of words that compose a review. The learning task consists in predicting the sentiment of the review.\n","In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like LSTM and GRU. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will explore how to face overfitting by Dropout. "]},{"cell_type":"markdown","metadata":{"id":"WTXPA4gUKkpp"},"source":["##[TO COMPLETE] Exercise 4.1: Simple RNN\n","\n","Let's start by importing Tensorflow, Keras and Numpy"]},{"cell_type":"code","metadata":{"id":"G-hX557NDQ9s"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZxFjnG7Kuwi"},"source":["###Load dataset:\n","In this HW, we use the same datset used in the HW2, the IMDB dataset. The dataset contains 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes. For convenience, the words are indexed by the overall frequency in the dataset, so that for example the integer \"3\" encodes the 3rd most frequent word in the data. For testing purposes, we will only consider the first 10,000  most common words.\n","By default, the load_data method returns a breakdown of the dataset into training and test sets. Both these sets contain 25,000 samples. To also have a validation set, we split the test set in half."]},{"cell_type":"code","metadata":{"id":"NpaX6AGOD77D"},"source":["\n","num_words = 10000\n","(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n","(X_valid, X_test) = X_test[:12500], X_test[12500:]\n","(y_valid, y_test) = y_test[:12500], y_test[12500:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZHxa95qQ0jb"},"source":["Let's also get the word indexs (word to word-id)"]},{"cell_type":"code","metadata":{"id":"LszZMh45X8Wz"},"source":["word_index = keras.datasets.imdb.get_word_index()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxLLVb5mRc0v"},"source":["Now we create a reverse index (word-id to word) method. Moreover, we add three special word-ids to encode:\n","- the padding;\n","- the start of a sequence;\n","- a word that is not in the vocabulary of the first 10,000 most common words.\n","\n","Moreover, we also add an \"unknown\" placeholder for all the other symbols (not words) that may occur.\n","*Please, notice that Keras does not use index 0, so we can shift the indices only by 3 positions.*"]},{"cell_type":"code","metadata":{"id":"NeZwDtDjYFt3"},"source":["reverse_index = {word_id + 3: word for word, word_id in word_index.items()}\n","reverse_index[0] = \"<pad>\" # padding\n","reverse_index[1] = \"<sos>\" # start of sequence\n","reverse_index[2] = \"<oov>\" # out-of-vocabulary\n","reverse_index[3] = \"<unk>\" # unknown\n","\n","def decode_review(word_ids):\n","    return \" \".join([reverse_index.get(word_id, \"<err>\") for word_id in word_ids])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIMY115nRzXQ"},"source":["Let's print a training sample and its target value"]},{"cell_type":"code","metadata":{"id":"jnBRMiZJYQpm"},"source":["decode_review(X_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1tzUWldYT8X"},"source":["y_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0O7x_P0IqihK"},"source":["Because of a limit of Keras, to feed the input data into an RNN model we have to create sequences that have the same length. We use keras.preprocessing.sequence.pad_sequences() to preprocess X_train: this will create a 2D array of 25,000 rows (one per review) and maxlen=500 columns. Because of that, reviews longer than 500 words will be cut, while reviews shorter than 500 words will be padded with zeros."]},{"cell_type":"code","metadata":{"id":"wtMUMGMUYdAA"},"source":["maxlen = 500\n","X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n","X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n","X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jLxP0KW1104"},"source":["###[TO COMPLETE] Define the model:\n","Let's define the model: \n","- The first layer is an Embedding layer, with input_dim=num_words and output_dim=10. The model will gradually learn to represent each of the 10,000 words as a 10-dimensional vector. So the next layer will receive 3D batches of shape (batch size, 500, 10)\n","- The second layer is the recurrent one. In particular, in this case, we use a [SimpleRNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) \n","- The output layer is a Dense layer with a sigmoid activation function since this is a binary classification problem\n","\n","[TO COMPLETE] In the following cell, we already inserted in the model the first Embedding layer. Add the recurrent layer (using 32 units), and the output layer. Select the right activation function for the output layer and motivate your choice. Finally, select the right loss function inserting the right value for the \"loss\" parameter in model.compile() and motivate your choice.\n","Please, insert your answers in a new text cell below this one, immediately before the code."]},{"cell_type":"code","metadata":{"id":"xPnihHKiYf__"},"source":["model = keras.models.Sequential()\n","model.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model.add([TO COMPLETE])\n","model.add([TO COMPLETE])\n","\n","model.compile(loss=[TO COMPLETE], optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9IBNh_o2I2SN"},"source":["Let's print a summary of the model. Specifically, note the number of parameters of the RNN layer."]},{"cell_type":"code","metadata":{"id":"6AKJ1qsgYkH7"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTrvel3KRkYy"},"source":["###Train the model:\n","Now we have to train the model"]},{"cell_type":"code","metadata":{"id":"zCqSgX9WYmaR"},"source":["history = model.fit(X_train_trim, y_train,\n","                    epochs=10, batch_size=128, validation_data=(X_valid_trim, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Fvscg67R4H1"},"source":["Print the values of accuracy and the loss , and evaluate the model on the test set"]},{"cell_type":"code","metadata":{"id":"GKXRHieZbu-2"},"source":["def plot_loss(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['loss'])\n","  plt.plot(history.epoch,history.history['val_loss'])\n","  plt.title('loss')\n","\n","def plot_accuracy(history):\n","  plt.figure(figsize=(10,6))\n","  plt.plot(history.epoch,history.history['accuracy'])\n","  plt.plot(history.epoch,history.history['val_accuracy'])\n","  plt.title('accuracy')\n","\n","plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHc_4yOASBXu"},"source":["##[TO COMPLETE] Exercise 4.2: LSTM and GRU\n","**[TO COMPLETE]**: In this Exercise, you have to implement 2 models, similar to the previous one that, instead of exploiting the RNN layer, use an LSTM and a GRU Layer, respectively. For each model print the summary. Then, train it and plot the values of accuracy and loss. Finally, discuss the differences in terms of performance, the number of parameters, and training time. Note that you can use a different number of units than the one used in the RNN example.\n","\n","**[TO COMPLETE]**: In order to perform a fair comparison (definition of fair: models have to use more or less the same number of parameters) between the given RNN model and the other 2 models (LSTM and GRU), how many units do they have to use, respectively?\n","\n","*insert cells (code and text) with results and discussion immediately after this cell*"]},{"cell_type":"markdown","metadata":{"id":"g2JZMtVATw5T"},"source":["##Bidirectional LSTM\n","Let's modify the previous code by using a bidirectional LSTM instead of a simple LSTM. In Keras, it is possible to define a bidirectional layer by using [tf.keras.layers.Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional). Note that this wrapper requires as argument a layer, in our case we use [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n"]},{"cell_type":"code","metadata":{"id":"svybeLNAedBf"},"source":["model_bidirectional = keras.models.Sequential()\n","model_bidirectional.add(keras.layers.Embedding(input_dim=num_words, output_dim=10))\n","model_bidirectional.add(keras.layers.Bidirectional(keras.layers.LSTM(32)))\n","model_bidirectional.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n","\n","model_bidirectional.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model_bidirectional.summary()\n","\n","history = model_bidirectional.fit(X_train_trim, y_train,\n","                    epochs=5, batch_size=128, validation_data=(X_valid_trim, y_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qICZr3WpfNI8"},"source":["plot_loss(history)\n","\n","plot_accuracy(history)\n","\n","scores = model_bidirectional.evaluate(X_test_trim, y_test, verbose=2)\n","print(\"%s: %.2f%%\" % (model_bidirectional.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8hmkhTciu89"},"source":["##[TO COMPLETE] Exercise 4.3: Dropout\n","In the bi-directional model is it possible to notice that the model overfits the training data.\n","A possible solution to this problem could be applying the dropout technique to input or output layers.\n","\n","In Keras, to apply dropout you have to add a [keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) layer.\n","\n","**[TO COMPLETE]**: modify the code of the previous exercise adding the Dropout to the input and/or output layers. Find the Dropout probability that reduces the overfitting issue.\n","\n","**[TO COMPLETE]** Is it possible to modify the values of some other hyper-parameters to mitigate overfitting? And if this is the case, explain why and perform an experimental study demonstrating your point.\n","\n","*insert cells (code and text) with results and discussion immediately after this cell*"]}]}