# -*- coding: utf-8 -*-
"""HW5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQuHb5xkb9poHO6_KSBkQlas8FV2UR2H

#HW5: Autoencoders

In this homework, we will explore how to develop a simple Autoencoder. As a dataset, we will use the MNIST dataset. It contains handwritten digits images.
In the first part, we will learn how to develop a simple shallow autoencoder, then we will develop a deep version. Next, we will experiment with the application of autoencoder on denoising data task (denoising-autoencoder). Finally, we will apply this model to sequential domains, considering the IMDB dataset, already used in HW4.
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

"""###Load Dataset
We load the MNIST dataset, using tf.keras.datasets. The dataset contains 60,000 training images and 10,000 testing images.
The value of each pixel is between 0 and 255, and it represents a point of an image of size 28 x 28. We will normalize all values between 0 and 1, and we will flatten the 28x28 images into vectors of size 784.
Finally, since no validation set is defined, we split the test set in a validation set and a new test set.
"""

"""##[TO COMPLETE] Exercise 5.1: Singular Value Decomposition

Principal component analysis (PCA) and singular value decomposition (SVD) are both classical linear dimensionality reduction methods that attempt to find linear combinations of features in the original high dimensional data matrix to construct a meaningful representation of the dataset.
In this first part of the HW, we will focus our attention on SVD decomposition, a numerical stable method. Given a matrix X, the SVD decomposes it into the product of two unitary matrices V and U and a rectangular diagonal matrix of singular values S:

$$ X=V \cdot S \cdot U^T.$$

SVD is already implemented in NumPy as np.linalg.svd. In our case, the X matrix will represent the training set, where each row is a sample (therefore the number of columns will be the number of input features).

Note that, the X matrix in our case will have a huge number of rows (we have 50000 input samples) and only 784 columns. Therefore to optime the memory consumption, we can compute the SVD of the covariance matrix. An interesting property of the SVD is that we compute the decomposition of the covariance matrix $C= X^T \cdot X$, and we will obtain the following decomposition:

$$ C= U \cdot S^2 \cdot U^T$$

Since we need just the matrix U to compute the compressed version of our data, this method will be very convenient. If you are using the collab free plan, the quantity of available ram is not sufficient to compute the SVD of X, therefore computing the SVD of the covariance matrix turns out to be the best solution. 
"""

"""## Exercise 5.6: Linear Autoencoder for sequences

Let's define a linear autoencoder for sequences. In this case, as dataset, we will use the IMDB dataset (already presented in HW4). To have a model that can be trained and tested in a reasonable time (and that works also with the memory limitation that we have in Colab), we will limit the number of training samples and test samples.
"""


def plot_loss(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.epoch, history.history['loss'])
    plt.plot(history.epoch, history.history['val_loss'])
    plt.title('loss')


num_words = 100
(X_train, _), (X_test, _) = keras.datasets.imdb.load_data(num_words=num_words)

X_train = X_train[:10000]

(X_valid, X_test) = X_test[:1250], X_test[-1250:]

word_index = keras.datasets.imdb.get_word_index()

reverse_index = {word_id + 3: word for word, word_id in word_index.items()}
reverse_index[0] = "<pad>"  # padding
reverse_index[1] = "<sos>"  # start of sequence
reverse_index[2] = "<oov>"  # out-of-vocabulary
reverse_index[3] = "<unk>"  # unknown


def decode_review(word_ids):
    return " ".join([reverse_index.get(word_id, "<err>") for word_id in word_ids])


maxlen = 90
X_train_trim = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)
X_test_trim = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)
X_valid_trim = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=maxlen)

"""In this case, we want to use as input/target a one-hot representation for each word. To convert the index representation provided by IMDB dataset loader we use the to_categorical method to transform them in the corresponding one hot representation."""

from keras.utils import to_categorical

X_train_one_hot = to_categorical(X_train_trim)
X_test_one_hot = to_categorical(X_test_trim)
X_valid_one_hot = to_categorical(X_valid_trim)

"""Define a linear shallow autoencoder for sequences. The structure will be similar to the model defined in Exercise 5.2, while the used encoding layer is defined by using tf.keras.layers.SimpleRNN. Note that it uses linear activations. The decoding layer exploits [tf.keras.layers.TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed) that allows using the same dense cell at each time step of the sequence. """

"""Let's plot the accuracy and the loss trends and check the reconstruction capability of the model by plotting the reconstruction of a test sample"""


def plot_categorical_accuracy(history):
    plt.figure(figsize=(10, 6))
    plt.plot(history.epoch, history.history['categorical_accuracy'])
    plt.plot(history.epoch, history.history['val_categorical_accuracy'])
    plt.title('accuracy')


"""##[TO COMPLETE] Exercise 5.7: Non-Linear Autoencoder for sequences
**[TO COMPLETE]**: Replicate the code of the above exercise, but instead of using a simpleRNN with linear activation do the same  using  non-linear activation functions and using an LSTM layer. Choose the most appropriate non-linear function, and motivate your choice. Then discuss the results in relation to those obtained by the linear autoencoder for sequences.

Hint: using a non-linear function also in the dense layer after the RNN/LSTM one will help to obtain better results. The choice of this function should be based on the type of output data.
"""
acts = ['tanh', 'sigmoid', 'relu', 'linear']
for act in acts:
    inputs = tf.keras.layers.Input(shape=(maxlen, num_words))
    encoded = tf.keras.layers.LSTM(50, return_sequences=True, activation=act)(inputs)
    decoded = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_words, activation='tanh'))(encoded)

    sequence_autoencoder = tf.keras.models.Model(inputs, decoded)
    encoder = tf.keras.models.Model(inputs, encoded)
    sequence_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',
                             metrics=["CategoricalAccuracy"])
    history = sequence_autoencoder.fit(X_train_one_hot, X_train_one_hot, epochs=50, batch_size=128, shuffle=True,
                                   validation_data=(X_valid_one_hot, X_valid_one_hot))
    plot_loss(history)
    plot_categorical_accuracy(history)
    print('act: ', act)
    scores = sequence_autoencoder.evaluate(X_test_one_hot, X_test_one_hot, verbose=2)
    print("%s: %.2f%%" % (sequence_autoencoder.metrics_names[1], scores[1] * 100))
    decoded_text = sequence_autoencoder.predict(X_test_one_hot)
    decode_index = np.argmax(decoded_text[500], axis=1)
    input_text = np.argmax(X_test_one_hot[500], axis=1)
    print(decode_review(input_text))
    print(decode_review(decode_index))
